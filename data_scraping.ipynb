{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_scraping",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSqg6u9Z-7a_"
      },
      "source": [
        "import requests\r\n",
        "from IPython.core.display import HTML\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import datetime\r\n",
        "import os\r\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nu7u2OVCX8i"
      },
      "source": [
        "#mount drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTtVqyR3CaQC"
      },
      "source": [
        "filename=\"/content/gdrive/My Drive/GG_Project_NLP/Data/commit_messages/json_data.csv\"\r\n",
        "with open(\"{}\".format(filename), 'r') as csvfile:\r\n",
        "  df=pd.read_csv(csvfile)\r\n",
        "\r\n",
        "df[\"project\"]=df[\"project\"].astype(str)\r\n",
        "\r\n",
        "project_list=df[\"project\"].unique().astype(str)\r\n",
        "project_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRWc40hCCcI_"
      },
      "source": [
        "u = \"http://mail-archives.apache.org/mod_mbox/\"\r\n",
        "r = requests.get(u)\r\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\r\n",
        "cols = soup.find_all(\"td\")\r\n",
        "pat = re.compile(\"Author:\")\r\n",
        "pat2 = re.compile(r'\\<(.*?)\\>')\r\n",
        "#author = set()\r\n",
        "#email = set()\r\n",
        "commit_data = pd.DataFrame(columns = ['proj_name', 'date_time', 'author_name', 'email_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtibo7wFCeFV"
      },
      "source": [
        "for i in range(len(cols)):\r\n",
        "  #choosing projects\r\n",
        "  proj = cols[i].ul.contents    #enter column index/ loop throught the three columns\r\n",
        "  for j in range(len(proj)):\r\n",
        "    author = \"\"\r\n",
        "    email = \"\"\r\n",
        "    if proj[j] != '\\n':\r\n",
        "      proj_name = proj[j].a.get(\"name\").split(\".\")[0]   #enter row index/ loop through the rows\r\n",
        "      print(proj_name)\r\n",
        "      if proj_name in project_list: #only scrape 272 projects present in json file\r\n",
        "        print(proj_name)\r\n",
        "        commit_list = [comm for comm in proj[j].contents[1].text.split() if 'commits' in comm]         \r\n",
        "        for commit in commit_list:\r\n",
        "          #getting commits - try changing this to get dev, users, etc\r\n",
        "          \r\n",
        "          u2 = \"http://mail-archives.apache.org/mod_mbox/{}-{}/\".format(proj_name, commit)\r\n",
        "          r2 = requests.get(u2)\r\n",
        "          soup2 = BeautifulSoup(r2.text, 'html.parser')\r\n",
        "\r\n",
        "          #choosing the year\r\n",
        "          proj2 = soup2.find(\"table\", attrs = {\"id\" : \"grid\"})\r\n",
        "          tabs = proj2.find_all(\"table\", class_ = \"year\")\r\n",
        "\r\n",
        "          for k in range(len(tabs)):\r\n",
        "            link = tabs[k].tbody.find_all(\"tr\")   #change this index to get different years\r\n",
        "\r\n",
        "            for y in range(len(link)):\r\n",
        "              \r\n",
        "              mon = link[y].find(\"span\", class_ = \"links\").a.get(\"href\")\r\n",
        "              mon2 = mon.split(\"/\")[0]\r\n",
        "              mon = mon2+\"/thread\"\r\n",
        "              #print(mon[:4], mon[4:6])\r\n",
        "              nurl = \"http://mail-archives.apache.org/mod_mbox/{}-{}/{}\".format(proj_name, commit, mon)\r\n",
        "              date = \"_\".join(link[y].td.text.split())\r\n",
        "              r3 = requests.get(nurl)\r\n",
        "              soup3 = BeautifulSoup(r3.text, 'lxml')\r\n",
        "              pages= soup3.body.find(\"table\", id = \"msglist\").thead.find(\"th\", class_ = \"pages\").find_all(\"a\")\r\n",
        "\r\n",
        "              if (len(pages)>0):\r\n",
        "                  page_list=[\"http://mail-archives.apache.org/mod_mbox/{}-{}/{}/thread?{}\".format(proj_name, commit, mon.split(\"/\")[0], thread) for thread in range(len(pages))]\r\n",
        "              else:\r\n",
        "                  page_list=[\"http://mail-archives.apache.org/mod_mbox/{}-{}/{}\".format(proj_name, commit, mon)]\r\n",
        "\r\n",
        "              for page in page_list:\r\n",
        "                nurl = page\r\n",
        "                r3 = requests.get(nurl)\r\n",
        "                soup3 = BeautifulSoup(r3.text, 'lxml')\r\n",
        "                coms = soup3.body.find(\"table\", id = \"msglist\").tbody.find_all(\"tr\")\r\n",
        "                for c in range(len(coms)):\r\n",
        "                #getting different commits and saving them\r\n",
        "                  if coms[c].find(\"td\", class_ = \"subject\").a:\r\n",
        "                    string = coms[c].find(\"td\", class_ = \"subject\").a.get(\"href\")\r\n",
        "                    string = string.replace(\"%3c\", \"<\")\r\n",
        "                    string = string.replace(\"%3e\", \">\")\r\n",
        "                    link2 = string\r\n",
        "                    #link2 = \"<\" + coms[c].find(\"td\", class_ = \"subject\").a.get(\"href\")[3:-3] + \">\"\r\n",
        "                    nurl2 = u2 + mon2 + \"/\" + link2\r\n",
        "                    r4 = requests.get(nurl2)\r\n",
        "                    soup4 = BeautifulSoup(r4.text, 'lxml')\r\n",
        "                    #soup4.tbody.find_all(\"td\", class_ = \"right\")\r\n",
        "                    #soup4.tbody.find(\"tr\", class_ = \"contents\").pre\r\n",
        "                    te = soup4.tbody.find(\"tr\", class_ = \"contents\").pre.text\r\n",
        "                    if 'Author:' in te:\r\n",
        "                      split_te = te.split('\\n')\r\n",
        "                      auth = list(filter(pat.search, split_te))[0]\r\n",
        "                      ind_1 = pat.search(auth).span()[1] + 1\r\n",
        "                      date_0 = coms[c].find(\"td\", class_ = \"date\").text.split(',')\r\n",
        "                      date_0.insert(2, mon[:4])\r\n",
        "                      date_0 = (\",\").join(date_0)\r\n",
        "                      date_time_str = str(date_0)\r\n",
        "                      date_time_obj = datetime.datetime.strptime(date_time_str, '%a, %d %b,%Y, %H:%M')\r\n",
        "                      #date = \"_\".join(str(date_time_obj).split())\r\n",
        "                      if '<' in auth:\r\n",
        "                        bind_1 = pat2.search(auth).span()[0] \r\n",
        "                        bind_2 = pat2.search(auth).span()[1]\r\n",
        "                        email = auth[bind_1:bind_2]\r\n",
        "                        author = auth[ind_1:bind_1]\r\n",
        "                      else:\r\n",
        "                        author = auth[ind_1:]\r\n",
        "                        email = \"\"\r\n",
        "                      commit_dict = {'proj_name' : proj_name, 'date_time' : date_time_obj, 'author_name' : author, 'email_id' : email}\r\n",
        "                      commit_data = commit_data.append(commit_dict, ignore_index=True)\r\n",
        "          \r\n",
        "          commit_data.to_csv(\"/content/gdrive/My Drive/GG_Project_NLP/Data/commit_messages/all_pages_author_email/commit_data_iv_{}_{}.csv\".format(i),j))\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SCCKesECu-8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}